ğŸ“˜ Synthetic AI SOC

AI-powered Synthetic Security Operations Platform â€” LLM-driven analyst co-pilot with synthetic telemetry, anomaly detection, and attack simulation.
Zero enterprise data. Fully open-source.

ğŸš€ Overview

Synthetic AI SOC is a modular, open-source platform that simulates enterprise-like telemetry and security attacks, applies ML/UEBA-based anomaly detection, and uses LLMs to generate high-quality analyst insights and triage summaries.

It allows you to demonstrate end-to-end expertise in:

- AI/ML for threat detection
- UEBA & anomaly modeling
- LLM reasoning for SOC analysts
- Real-time ingestion & backend architectures
- Detection engineering & behavioral security
- Full platform/system design

All data is synthetic, generated entirely by the platform's simulation engine. No enterprise or sensitive data is used.

ğŸ— Architecture (MVP)
```
[Synthetic Log Simulator] 
     â†’ [Ingestion Layer] 
     â†’ [Stream Processor] 
     â†’ [Feature Store + Embeddings] 
     â†’ [Anomaly Scoring Service] 
     â†’ [Alert Store] 
     â†’ [LLM Reasoner / RAG] 
     â†’ [Web UI Dashboard]
```

âœ¨ Key Features (MVP)

- **Synthetic log generator** (auth, process, network, MITRE-style attack sequences)
- **Real-time ingestion pipeline** (Kafka or simple queue fallback)
- **ML-based anomaly detection** (Isolation Forest with 74% recall, F1=0.53)
- **Alerts API** (FastAPI) + SQLite/Postgres store
- **LLM-powered explanation service** (RAG + structured incident summaries)
- **React dashboard** for viewing alerts in real-time
- **Model benchmarking** (Precision, Recall, F1, ROC-AUC metrics)

ğŸ¤– ML Models

The platform includes multiple anomaly detection models:

| Model | Status | Performance | Use Case |
|-------|--------|-------------|----------|
| **Isolation Forest** | âœ… Default | F1=0.53, Recall=74% | General anomaly detection |
| LOF | Available | F1=0.00 | Experimental |
| One-Class SVM | Available | F1=0.00 | Experimental |
| Ensemble | Available | F1=0.00 | Experimental |

**Isolation Forest** was selected as the default after extensive benchmarking. See `docs/model_config.md` for details.

ğŸ“‚ Project Structure
```
synthetic-ai-soc/
â”œâ”€ simulator/                 # Synthetic log & attack simulator
â”œâ”€ ingestion/                 # Producers & connectors
â”œâ”€ stream-processing/         # Feature extraction jobs
â”œâ”€ feature-store/             # Redis/Vector embeddings
â”œâ”€ anomaly-service/           # ML scoring microservice (FastAPI)
â”œâ”€ llm-reasoner/              # RAG, prompts, LLM explanations
â”œâ”€ alert-store/               # SQLite/Postgres schema + API
â”œâ”€ graph-db/                  # Optional: attack path graph (v1.0+)
â”œâ”€ ui/                        # React dashboard
â”œâ”€ infra/                     # Docker/K8s deployment configs
â”œâ”€ docs/                      # Architecture, benchmarks, model config
â””â”€ scripts/                   # Training, benchmarking, demo scripts
```

ğŸ§ª Quickstart (local dev)

**Option 1: Docker Compose (Full Stack)**
```bash
docker-compose up --build
```

**Option 2: Local Development**
```bash
# Terminal 1: Start anomaly service
cd anomaly-service
python3 -m uvicorn app:app --reload --port 8001

# Terminal 2: Run simulator and ingestion
python3 simulator/sim_generator.py | python3 ingestion/ingest_and_score.py

# Terminal 3: Start UI (requires Node.js)
cd ui/webapp
npm install
npm run dev
```

Then open: ğŸ‘‰ **http://localhost:3000** to view alerts in the dashboard.

ğŸ”¬ ML Training & Benchmarking

**Train the model on real data:**
```bash
python3 scripts/train_model.py
```

**Run benchmarks:**
```bash
python3 scripts/benchmark_model.py
```

**Use alternative models:**
```bash
MODEL=lof python3 scripts/benchmark_model.py
```

ğŸ¤– LLM Reasoner (triage)
```bash
# Terminal 1: start LLM reasoner
cd llm-reasoner
python3 -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
LLM_HOST=127.0.0.1 LLM_PORT=8002 uvicorn service:app --log-level warning

# Terminal 2: health check
TRIAGE_URL=http://127.0.0.1:8002/health python llm-reasoner/scripts/check_health.py

# Terminal 2: exercise triage endpoint (rule-based or OpenAI if OPENAI_API_KEY is set)
TRIAGE_URL=http://127.0.0.1:8002 python llm-reasoner/scripts/test_triage.py
```

ğŸ›£ Roadmap

**v0.1 (MVP)** âœ…
- âœ… Simulator + ingestion
- âœ… Anomaly detection service (Isolation Forest)
- âœ… Alerts API
- âœ… ML benchmarking framework
- âœ… React UI dashboard
- ğŸš§ LLM explanations (basic placeholder)

**v1.0**
- Graph DB for identity + attack path reasoning
- GNN-based behavioral detection
- LLM enrichment playbooks
- Policy-as-code (YAML) alert rules

**v2.0**
- Plugin ecosystem
- Advanced MITRE ATT&CK simulation
- Dataset export for ML research
- Cloud-native deployment templates

ğŸ“Š Performance Metrics

Current anomaly detection performance (Isolation Forest):
- **Precision**: 41.6%
- **Recall**: 74%
- **F1 Score**: 0.53
- **ROC-AUC**: 0.70

See `docs/model_comparison.md` for detailed benchmarks.

ğŸ¤ Contributing

Contributions, ideas, PRs, and issues are welcome.
This project aims to become a useful security research and learning platform for the community.

ğŸ“š Documentation

- `docs/architecture.md` - System architecture
- `docs/model_config.md` - ML model configuration
- `docs/model_comparison.md` - Model benchmarking results
- `docs/ml_deep_dive.md` - ML implementation details
